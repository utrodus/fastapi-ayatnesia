[
    {
        "label": "sqlite3",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "sqlite3",
        "description": "sqlite3",
        "detail": "sqlite3",
        "documentation": {}
    },
    {
        "label": "os",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "os",
        "description": "os",
        "detail": "os",
        "documentation": {}
    },
    {
        "label": "psutil",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "psutil",
        "description": "psutil",
        "detail": "psutil",
        "documentation": {}
    },
    {
        "label": "dataclass",
        "importPath": "dataclasses",
        "description": "dataclasses",
        "isExtraImport": true,
        "detail": "dataclasses",
        "documentation": {}
    },
    {
        "label": "Any",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "List",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "TypeVar",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Type",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "cast",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "Callable",
        "importPath": "typing",
        "description": "typing",
        "isExtraImport": true,
        "detail": "typing",
        "documentation": {}
    },
    {
        "label": "json",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "json",
        "description": "json",
        "detail": "json",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "word_tokenize",
        "importPath": "nltk.tokenize",
        "description": "nltk.tokenize",
        "isExtraImport": true,
        "detail": "nltk.tokenize",
        "documentation": {}
    },
    {
        "label": "re",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "re",
        "description": "re",
        "detail": "re",
        "documentation": {}
    },
    {
        "label": "StopWordRemoverFactory",
        "importPath": "Sastrawi.StopWordRemover.StopWordRemoverFactory",
        "description": "Sastrawi.StopWordRemover.StopWordRemoverFactory",
        "isExtraImport": true,
        "detail": "Sastrawi.StopWordRemover.StopWordRemoverFactory",
        "documentation": {}
    },
    {
        "label": "StopWordRemoverFactory",
        "importPath": "Sastrawi.StopWordRemover.StopWordRemoverFactory",
        "description": "Sastrawi.StopWordRemover.StopWordRemoverFactory",
        "isExtraImport": true,
        "detail": "Sastrawi.StopWordRemover.StopWordRemoverFactory",
        "documentation": {}
    },
    {
        "label": "StopWordRemoverFactory",
        "importPath": "Sastrawi.StopWordRemover.StopWordRemoverFactory",
        "description": "Sastrawi.StopWordRemover.StopWordRemoverFactory",
        "isExtraImport": true,
        "detail": "Sastrawi.StopWordRemover.StopWordRemoverFactory",
        "documentation": {}
    },
    {
        "label": "StemmerFactory",
        "importPath": "Sastrawi.Stemmer.StemmerFactory",
        "description": "Sastrawi.Stemmer.StemmerFactory",
        "isExtraImport": true,
        "detail": "Sastrawi.Stemmer.StemmerFactory",
        "documentation": {}
    },
    {
        "label": "StemmerFactory",
        "importPath": "Sastrawi.Stemmer.StemmerFactory",
        "description": "Sastrawi.Stemmer.StemmerFactory",
        "isExtraImport": true,
        "detail": "Sastrawi.Stemmer.StemmerFactory",
        "documentation": {}
    },
    {
        "label": "StopwordList",
        "importPath": "stopword_list",
        "description": "stopword_list",
        "isExtraImport": true,
        "detail": "stopword_list",
        "documentation": {}
    },
    {
        "label": "JSONReader",
        "importPath": "json_reader",
        "description": "json_reader",
        "isExtraImport": true,
        "detail": "json_reader",
        "documentation": {}
    },
    {
        "label": "Preprocessing",
        "importPath": "preprocessing",
        "description": "preprocessing",
        "isExtraImport": true,
        "detail": "preprocessing",
        "documentation": {}
    },
    {
        "label": "data.database",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "data.database",
        "description": "data.database",
        "detail": "data.database",
        "documentation": {}
    },
    {
        "label": "math",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "math",
        "description": "math",
        "detail": "math",
        "documentation": {}
    },
    {
        "label": "numpy",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "numpy",
        "description": "numpy",
        "detail": "numpy",
        "documentation": {}
    },
    {
        "label": "pandas",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "pandas",
        "description": "pandas",
        "detail": "pandas",
        "documentation": {}
    },
    {
        "label": "Preprocessing",
        "importPath": "preprocessing.preprocessing",
        "description": "preprocessing.preprocessing",
        "isExtraImport": true,
        "detail": "preprocessing.preprocessing",
        "documentation": {}
    },
    {
        "label": "Preprocessing",
        "importPath": "preprocessing.preprocessing",
        "description": "preprocessing.preprocessing",
        "isExtraImport": true,
        "detail": "preprocessing.preprocessing",
        "documentation": {}
    },
    {
        "label": "get_all_ayahs",
        "importPath": "data.database.database",
        "description": "data.database.database",
        "isExtraImport": true,
        "detail": "data.database.database",
        "documentation": {}
    },
    {
        "label": "test_connections",
        "importPath": "data.database.database",
        "description": "data.database.database",
        "isExtraImport": true,
        "detail": "data.database.database",
        "documentation": {}
    },
    {
        "label": "get_all_surahs",
        "importPath": "data.database.database",
        "description": "data.database.database",
        "isExtraImport": true,
        "detail": "data.database.database",
        "documentation": {}
    },
    {
        "label": "get_all_ayahs_by_surah_id",
        "importPath": "data.database.database",
        "description": "data.database.database",
        "isExtraImport": true,
        "detail": "data.database.database",
        "documentation": {}
    },
    {
        "label": "ArrayDictionary",
        "importPath": "Sastrawi.Dictionary.ArrayDictionary",
        "description": "Sastrawi.Dictionary.ArrayDictionary",
        "isExtraImport": true,
        "detail": "Sastrawi.Dictionary.ArrayDictionary",
        "documentation": {}
    },
    {
        "label": "StopWordRemover",
        "importPath": "Sastrawi.StopWordRemover.StopWordRemover",
        "description": "Sastrawi.StopWordRemover.StopWordRemover",
        "isExtraImport": true,
        "detail": "Sastrawi.StopWordRemover.StopWordRemover",
        "documentation": {}
    },
    {
        "label": "string",
        "kind": 6,
        "isExtraImport": true,
        "importPath": "string",
        "description": "string",
        "detail": "string",
        "documentation": {}
    },
    {
        "label": "FastAPI",
        "importPath": "fastapi",
        "description": "fastapi",
        "isExtraImport": true,
        "detail": "fastapi",
        "documentation": {}
    },
    {
        "label": "test_connections",
        "kind": 2,
        "importPath": "src.data.database.database",
        "description": "src.data.database.database",
        "peekOfCode": "def test_connections():\n    for proc in psutil.process_iter():\n        try:\n            files = proc.get_open_files()\n            if files:\n                for _file in files:\n                    if _file.path == quran_preprocessed_db_path:\n                        return True\n        except psutil.NoSuchProcess as err:\n            print(err)",
        "detail": "src.data.database.database",
        "documentation": {}
    },
    {
        "label": "write_quran_db",
        "kind": 2,
        "importPath": "src.data.database.database",
        "description": "src.data.database.database",
        "peekOfCode": "def write_quran_db(surahs):\n    # Create the surah table\n    c.execute(\n        \"\"\"CREATE TABLE surah (\n                id INTEGER PRIMARY KEY,             \n                name TEXT,\n                translation TEXT\n                )\"\"\"\n    )\n    # Create the ayahs table",
        "detail": "src.data.database.database",
        "documentation": {}
    },
    {
        "label": "get_all_surahs",
        "kind": 2,
        "importPath": "src.data.database.database",
        "description": "src.data.database.database",
        "peekOfCode": "def get_all_surahs():\n    c.execute(\"SELECT * FROM surah\")\n    all_surahs = c.fetchall()\n    result = []\n    for item in all_surahs:\n        dictionary = {\n            \"id\": item[0],            \n            \"name\": item[1],\n            \"translation\": item[2]\n        }",
        "detail": "src.data.database.database",
        "documentation": {}
    },
    {
        "label": "get_all_ayahs_by_surah_id",
        "kind": 2,
        "importPath": "src.data.database.database",
        "description": "src.data.database.database",
        "peekOfCode": "def get_all_ayahs_by_surah_id(surah_id):\n    c.execute(\"SELECT * FROM ayahs WHERE surah_id = ?\", (surah_id,))\n    all_ayahs = c.fetchall()\n    result = []\n    for item in all_ayahs:\n        dictionary = {\n            \"id\": item[0],\n            \"surah_id\": item[1],\n            \"number\": {\n                \"inQuran\": item[2],",
        "detail": "src.data.database.database",
        "documentation": {}
    },
    {
        "label": "get_all_ayahs",
        "kind": 2,
        "importPath": "src.data.database.database",
        "description": "src.data.database.database",
        "peekOfCode": "def get_all_ayahs():\n    c.execute(\"SELECT * FROM ayahs\")\n    all_ayahs = c.fetchall()\n    result = []\n    for item in all_ayahs:\n        dictionary = {\n            \"id\": item[0],\n            \"surah_id\": item[1],\n            \"number\": {\n                \"inQuran\": item[2],",
        "detail": "src.data.database.database",
        "documentation": {}
    },
    {
        "label": "current_path",
        "kind": 5,
        "importPath": "src.data.database.database",
        "description": "src.data.database.database",
        "peekOfCode": "current_path = os.path.join(os.path.dirname(os.path.realpath(__file__)))\nquran_preprocessed_db_path = os.path.abspath(\n    os.path.join(current_path, \"data/database/quran.db\")\n)\n# Connect to SQLite database\nconn = sqlite3.connect(quran_preprocessed_db_path)\nc = conn.cursor()\n# test connection\ndef test_connections():\n    for proc in psutil.process_iter():",
        "detail": "src.data.database.database",
        "documentation": {}
    },
    {
        "label": "quran_preprocessed_db_path",
        "kind": 5,
        "importPath": "src.data.database.database",
        "description": "src.data.database.database",
        "peekOfCode": "quran_preprocessed_db_path = os.path.abspath(\n    os.path.join(current_path, \"data/database/quran.db\")\n)\n# Connect to SQLite database\nconn = sqlite3.connect(quran_preprocessed_db_path)\nc = conn.cursor()\n# test connection\ndef test_connections():\n    for proc in psutil.process_iter():\n        try:",
        "detail": "src.data.database.database",
        "documentation": {}
    },
    {
        "label": "conn",
        "kind": 5,
        "importPath": "src.data.database.database",
        "description": "src.data.database.database",
        "peekOfCode": "conn = sqlite3.connect(quran_preprocessed_db_path)\nc = conn.cursor()\n# test connection\ndef test_connections():\n    for proc in psutil.process_iter():\n        try:\n            files = proc.get_open_files()\n            if files:\n                for _file in files:\n                    if _file.path == quran_preprocessed_db_path:",
        "detail": "src.data.database.database",
        "documentation": {}
    },
    {
        "label": "c",
        "kind": 5,
        "importPath": "src.data.database.database",
        "description": "src.data.database.database",
        "peekOfCode": "c = conn.cursor()\n# test connection\ndef test_connections():\n    for proc in psutil.process_iter():\n        try:\n            files = proc.get_open_files()\n            if files:\n                for _file in files:\n                    if _file.path == quran_preprocessed_db_path:\n                        return True",
        "detail": "src.data.database.database",
        "documentation": {}
    },
    {
        "label": "Audio",
        "kind": 6,
        "importPath": "src.data.models.models",
        "description": "src.data.models.models",
        "peekOfCode": "class Audio:\n    alafasy: str\n    ahmedajamy: str\n    husarymujawwad: str\n    minshawi: str\n    muhammadayyoub: str\n    muhammadjibreel: str\n    @staticmethod\n    def from_dict(obj: Any) -> 'Audio':\n        assert isinstance(obj, dict)",
        "detail": "src.data.models.models",
        "documentation": {}
    },
    {
        "label": "Image",
        "kind": 6,
        "importPath": "src.data.models.models",
        "description": "src.data.models.models",
        "peekOfCode": "class Image:\n    primary: str\n    secondary: str\n    @staticmethod\n    def from_dict(obj: Any) -> 'Image':\n        assert isinstance(obj, dict)\n        primary = from_str(obj.get(\"primary\"))\n        secondary = from_str(obj.get(\"secondary\"))\n        return Image(primary, secondary)\n    def to_dict(self) -> dict:",
        "detail": "src.data.models.models",
        "documentation": {}
    },
    {
        "label": "Sajda",
        "kind": 6,
        "importPath": "src.data.models.models",
        "description": "src.data.models.models",
        "peekOfCode": "class Sajda:\n    recommended: bool\n    obligatory: bool\n    @staticmethod\n    def from_dict(obj: Any) -> 'Sajda':\n        assert isinstance(obj, dict)\n        recommended = from_bool(obj.get(\"recommended\"))\n        obligatory = from_bool(obj.get(\"obligatory\"))\n        return Sajda(recommended, obligatory)\n    def to_dict(self) -> dict:",
        "detail": "src.data.models.models",
        "documentation": {}
    },
    {
        "label": "Meta",
        "kind": 6,
        "importPath": "src.data.models.models",
        "description": "src.data.models.models",
        "peekOfCode": "class Meta:\n    juz: int\n    page: int\n    manzil: int\n    ruku: int\n    hizb_quarter: int\n    sajda: Sajda\n    @staticmethod\n    def from_dict(obj: Any) -> 'Meta':\n        assert isinstance(obj, dict)",
        "detail": "src.data.models.models",
        "documentation": {}
    },
    {
        "label": "Number",
        "kind": 6,
        "importPath": "src.data.models.models",
        "description": "src.data.models.models",
        "peekOfCode": "class Number:\n    in_quran: int\n    in_surah: int\n    @staticmethod\n    def from_dict(obj: Any) -> 'Number':\n        assert isinstance(obj, dict)\n        in_quran = from_int(obj.get(\"inQuran\"))\n        in_surah = from_int(obj.get(\"inSurah\"))\n        return Number(in_quran, in_surah)\n    def to_dict(self) -> dict:",
        "detail": "src.data.models.models",
        "documentation": {}
    },
    {
        "label": "Kemenag",
        "kind": 6,
        "importPath": "src.data.models.models",
        "description": "src.data.models.models",
        "peekOfCode": "class Kemenag:\n    short: str\n    long: str\n    @staticmethod\n    def from_dict(obj: Any) -> 'Kemenag':\n        assert isinstance(obj, dict)\n        short = from_str(obj.get(\"short\"))\n        long = from_str(obj.get(\"long\"))\n        return Kemenag(short, long)\n    def to_dict(self) -> dict:",
        "detail": "src.data.models.models",
        "documentation": {}
    },
    {
        "label": "Tafsir",
        "kind": 6,
        "importPath": "src.data.models.models",
        "description": "src.data.models.models",
        "peekOfCode": "class Tafsir:\n    kemenag: Kemenag\n    quraish: str\n    jalalayn: str\n    @staticmethod\n    def from_dict(obj: Any) -> 'Tafsir':\n        assert isinstance(obj, dict)\n        kemenag = Kemenag.from_dict(obj.get(\"kemenag\"))\n        quraish = from_str(obj.get(\"quraish\"))\n        jalalayn = from_str(obj.get(\"jalalayn\"))",
        "detail": "src.data.models.models",
        "documentation": {}
    },
    {
        "label": "Ayah",
        "kind": 6,
        "importPath": "src.data.models.models",
        "description": "src.data.models.models",
        "peekOfCode": "class Ayah:\n    number: Number\n    arab: str\n    translation: str\n    audio: Audio\n    image: Image\n    tafsir: Tafsir\n    meta: Meta\n    @staticmethod\n    def from_dict(obj: Any) -> 'Ayah':",
        "detail": "src.data.models.models",
        "documentation": {}
    },
    {
        "label": "Bismillah",
        "kind": 6,
        "importPath": "src.data.models.models",
        "description": "src.data.models.models",
        "peekOfCode": "class Bismillah:\n    arab: str\n    translation: str\n    audio: Audio\n    @staticmethod\n    def from_dict(obj: Any) -> 'Bismillah':\n        assert isinstance(obj, dict)\n        arab = from_str(obj.get(\"arab\"))\n        translation = from_str(obj.get(\"translation\"))\n        audio = Audio.from_dict(obj.get(\"audio\"))",
        "detail": "src.data.models.models",
        "documentation": {}
    },
    {
        "label": "QuranModelItem",
        "kind": 6,
        "importPath": "src.data.models.models",
        "description": "src.data.models.models",
        "peekOfCode": "class QuranModelItem:\n    number: int\n    number_of_ayahs: int\n    name: str\n    translation: str\n    revelation: str\n    description: str\n    audio: str\n    bismillah: Bismillah\n    ayahs: List[Ayah]",
        "detail": "src.data.models.models",
        "documentation": {}
    },
    {
        "label": "from_str",
        "kind": 2,
        "importPath": "src.data.models.models",
        "description": "src.data.models.models",
        "peekOfCode": "def from_str(x: Any) -> str:\n    assert isinstance(x, str)\n    return x\ndef from_bool(x: Any) -> bool:\n    assert isinstance(x, bool)\n    return x\ndef from_int(x: Any) -> int:\n    assert isinstance(x, int) and not isinstance(x, bool)\n    return x\ndef to_class(c: Type[T], x: Any) -> dict:",
        "detail": "src.data.models.models",
        "documentation": {}
    },
    {
        "label": "from_bool",
        "kind": 2,
        "importPath": "src.data.models.models",
        "description": "src.data.models.models",
        "peekOfCode": "def from_bool(x: Any) -> bool:\n    assert isinstance(x, bool)\n    return x\ndef from_int(x: Any) -> int:\n    assert isinstance(x, int) and not isinstance(x, bool)\n    return x\ndef to_class(c: Type[T], x: Any) -> dict:\n    assert isinstance(x, c)\n    return cast(Any, x).to_dict()\ndef from_list(f: Callable[[Any], T], x: Any) -> List[T]:",
        "detail": "src.data.models.models",
        "documentation": {}
    },
    {
        "label": "from_int",
        "kind": 2,
        "importPath": "src.data.models.models",
        "description": "src.data.models.models",
        "peekOfCode": "def from_int(x: Any) -> int:\n    assert isinstance(x, int) and not isinstance(x, bool)\n    return x\ndef to_class(c: Type[T], x: Any) -> dict:\n    assert isinstance(x, c)\n    return cast(Any, x).to_dict()\ndef from_list(f: Callable[[Any], T], x: Any) -> List[T]:\n    assert isinstance(x, list)\n    return [f(y) for y in x]\n@dataclass",
        "detail": "src.data.models.models",
        "documentation": {}
    },
    {
        "label": "to_class",
        "kind": 2,
        "importPath": "src.data.models.models",
        "description": "src.data.models.models",
        "peekOfCode": "def to_class(c: Type[T], x: Any) -> dict:\n    assert isinstance(x, c)\n    return cast(Any, x).to_dict()\ndef from_list(f: Callable[[Any], T], x: Any) -> List[T]:\n    assert isinstance(x, list)\n    return [f(y) for y in x]\n@dataclass\nclass Audio:\n    alafasy: str\n    ahmedajamy: str",
        "detail": "src.data.models.models",
        "documentation": {}
    },
    {
        "label": "from_list",
        "kind": 2,
        "importPath": "src.data.models.models",
        "description": "src.data.models.models",
        "peekOfCode": "def from_list(f: Callable[[Any], T], x: Any) -> List[T]:\n    assert isinstance(x, list)\n    return [f(y) for y in x]\n@dataclass\nclass Audio:\n    alafasy: str\n    ahmedajamy: str\n    husarymujawwad: str\n    minshawi: str\n    muhammadayyoub: str",
        "detail": "src.data.models.models",
        "documentation": {}
    },
    {
        "label": "quran_model_from_dict",
        "kind": 2,
        "importPath": "src.data.models.models",
        "description": "src.data.models.models",
        "peekOfCode": "def quran_model_from_dict(s: Any) -> List[QuranModelItem]:\n    return from_list(QuranModelItem.from_dict, s)\ndef quran_model_to_dict(x: List[QuranModelItem]) -> Any:\n    return from_list(lambda x: to_class(QuranModelItem, x), x)",
        "detail": "src.data.models.models",
        "documentation": {}
    },
    {
        "label": "quran_model_to_dict",
        "kind": 2,
        "importPath": "src.data.models.models",
        "description": "src.data.models.models",
        "peekOfCode": "def quran_model_to_dict(x: List[QuranModelItem]) -> Any:\n    return from_list(lambda x: to_class(QuranModelItem, x), x)",
        "detail": "src.data.models.models",
        "documentation": {}
    },
    {
        "label": "T",
        "kind": 5,
        "importPath": "src.data.models.models",
        "description": "src.data.models.models",
        "peekOfCode": "T = TypeVar(\"T\")\ndef from_str(x: Any) -> str:\n    assert isinstance(x, str)\n    return x\ndef from_bool(x: Any) -> bool:\n    assert isinstance(x, bool)\n    return x\ndef from_int(x: Any) -> int:\n    assert isinstance(x, int) and not isinstance(x, bool)\n    return x",
        "detail": "src.data.models.models",
        "documentation": {}
    },
    {
        "label": "JSONReader",
        "kind": 6,
        "importPath": "src.preprocessing.json_reader",
        "description": "src.preprocessing.json_reader",
        "peekOfCode": "class JSONReader:\n    \"\"\" This class reads the JSON file and converts it to a list of dictionaries. \"\"\"\n    def __init__(self, file_path):\n        \"\"\"Init method for JSONReader class.\n        This constructor takes a file path as an argument and assigns it to the file_path attribute.\n        \"\"\"\n        self.file_path = file_path\n    def read_json_file(self):\n        \"\"\"Reads the JSON file and returns the data.\n        Returns:",
        "detail": "src.preprocessing.json_reader",
        "documentation": {}
    },
    {
        "label": "Preprocessing",
        "kind": 6,
        "importPath": "src.preprocessing.preprocessing",
        "description": "src.preprocessing.preprocessing",
        "peekOfCode": "class Preprocessing:\n    \"\"\" This class preprocesses the query or qur'an data. with the following steps:\n    1. Casefolding\n    2. Tokenization\n    3. Stopword Removal\n    4. Stemming\n    \"\"\"\n    def __init__(self, input_string):        \n        self.input_string = input_string\n        stopword_list = StopwordList()",
        "detail": "src.preprocessing.preprocessing",
        "documentation": {}
    },
    {
        "label": "get_quran_data",
        "kind": 2,
        "importPath": "src.preprocessing.quran_preprocessing",
        "description": "src.preprocessing.quran_preprocessing",
        "peekOfCode": "def get_quran_data():\n    \"\"\"Returns the data from the JSON file.\"\"\"\n    json_reader = JSONReader(quran_json_path)\n    quran_data = json_reader.convert_to_list_of_dict()\n    return quran_data\ndef preprocess_quran_data():\n    \"\"\"Preprocesses the qur'an data.\"\"\"\n    print(\"Preprocessing the qur'an data...\")\n    quran_data = get_quran_data()  # create a list of ayahs for each surah temporarily\n    temp_list_ayahs = []",
        "detail": "src.preprocessing.quran_preprocessing",
        "documentation": {}
    },
    {
        "label": "preprocess_quran_data",
        "kind": 2,
        "importPath": "src.preprocessing.quran_preprocessing",
        "description": "src.preprocessing.quran_preprocessing",
        "peekOfCode": "def preprocess_quran_data():\n    \"\"\"Preprocesses the qur'an data.\"\"\"\n    print(\"Preprocessing the qur'an data...\")\n    quran_data = get_quran_data()  # create a list of ayahs for each surah temporarily\n    temp_list_ayahs = []\n    # create a list of dictionaries from the qur'an data preprocessed\n    results = []\n    # iterate through the qur'an data for preprocessing and store the results in the results list\n    for surah in quran_data:\n        print(f\"## Preprocessing surat: {surah.name} \\n\")",
        "detail": "src.preprocessing.quran_preprocessing",
        "documentation": {}
    },
    {
        "label": "current_path",
        "kind": 5,
        "importPath": "src.preprocessing.quran_preprocessing",
        "description": "src.preprocessing.quran_preprocessing",
        "peekOfCode": "current_path = os.path.join(os.path.dirname(os.path.realpath(__file__)))\nquran_json_path = os.path.abspath(os.path.join(current_path, \"data/datasets/quran.json\"))\ndef get_quran_data():\n    \"\"\"Returns the data from the JSON file.\"\"\"\n    json_reader = JSONReader(quran_json_path)\n    quran_data = json_reader.convert_to_list_of_dict()\n    return quran_data\ndef preprocess_quran_data():\n    \"\"\"Preprocesses the qur'an data.\"\"\"\n    print(\"Preprocessing the qur'an data...\")",
        "detail": "src.preprocessing.quran_preprocessing",
        "documentation": {}
    },
    {
        "label": "quran_json_path",
        "kind": 5,
        "importPath": "src.preprocessing.quran_preprocessing",
        "description": "src.preprocessing.quran_preprocessing",
        "peekOfCode": "quran_json_path = os.path.abspath(os.path.join(current_path, \"data/datasets/quran.json\"))\ndef get_quran_data():\n    \"\"\"Returns the data from the JSON file.\"\"\"\n    json_reader = JSONReader(quran_json_path)\n    quran_data = json_reader.convert_to_list_of_dict()\n    return quran_data\ndef preprocess_quran_data():\n    \"\"\"Preprocesses the qur'an data.\"\"\"\n    print(\"Preprocessing the qur'an data...\")\n    quran_data = get_quran_data()  # create a list of ayahs for each surah temporarily",
        "detail": "src.preprocessing.quran_preprocessing",
        "documentation": {}
    },
    {
        "label": "StopwordList",
        "kind": 6,
        "importPath": "src.preprocessing.stopword_list",
        "description": "src.preprocessing.stopword_list",
        "peekOfCode": "class StopwordList:\n    \"\"\" This class contains the stopword list that will be used in the preprocessing step.\"\"\"\n    def __init__(self):\n        self.stopword_factory = StopWordRemoverFactory()\n        self.list_stopwords = self.stopword_factory.get_stop_words()\n        ignored_stopword_list = ['demi', 'masa', 'kamu',  'menjawab', 'bulan', 'bertanya-tanya', 'hari', 'besar', \n                                 'melihatnya', 'dia', 'tidak', 'sesuatu', 'sekali-kali', 'mengetahui', 'pasti', 'mengerjakan',\n                                 'nya', 'mereka', 'sendiri', 'bahwa', 'kami', 'melihat', 'lebih', 'dekat', 'kepadanya', 'daripada', 'apa',\n                                 'saling', 'mengira', 'dengan', 'jelas', 'hanyalah', 'ibu', 'bapak', 'mata', 'kadar', 'memberi'\n                               ]",
        "detail": "src.preprocessing.stopword_list",
        "documentation": {}
    },
    {
        "label": "hitung_tf_idf",
        "kind": 2,
        "importPath": "src.similarity_measure.lexical.experiment",
        "description": "src.similarity_measure.lexical.experiment",
        "peekOfCode": "def hitung_tf_idf(term, dokumen, korpus):\n    tf = dokumen.count(term) / len(dokumen.split())\n    idf = math.log(len(korpus) / (sum([1 for d in korpus if term in d])))\n    print(f\"{term}\\t\\tTF: {tf:.2f}\\tIDF: {idf:.2f}\")\n    return tf * idf\n# Fungsi untuk menghitung nilai bobot dari tiap kata pada suatu dokumen\ndef hitung_bobot_query(query, dokumen, korpus):\n    bobot = 0\n    print(f\"{'Kata':<15}{'TF-IDF':<15}\")\n    for term in query.split():",
        "detail": "src.similarity_measure.lexical.experiment",
        "documentation": {}
    },
    {
        "label": "hitung_bobot_query",
        "kind": 2,
        "importPath": "src.similarity_measure.lexical.experiment",
        "description": "src.similarity_measure.lexical.experiment",
        "peekOfCode": "def hitung_bobot_query(query, dokumen, korpus):\n    bobot = 0\n    print(f\"{'Kata':<15}{'TF-IDF':<15}\")\n    for term in query.split():\n        if term in dokumen:\n            tf_idf = hitung_tf_idf(term, dokumen, korpus)\n            bobot += tf_idf\n            print(f\"{term:<15}{tf_idf:<15.2f}\")\n        else:\n            print(f\"{term:<15}Tidak ditemukan pada dokumen\")",
        "detail": "src.similarity_measure.lexical.experiment",
        "documentation": {}
    },
    {
        "label": "cari_dokumen",
        "kind": 2,
        "importPath": "src.similarity_measure.lexical.experiment",
        "description": "src.similarity_measure.lexical.experiment",
        "peekOfCode": "def cari_dokumen(query, korpus):\n    skor_dokumen = []\n    print(f\"\\nMencari dokumen dengan query '{query}':\")\n    for i, dokumen in enumerate(korpus):\n        skor = hitung_bobot_query(query, dokumen, korpus)\n        skor_dokumen.append(skor)\n        print(f\"Skor dokumen {i+1}: {skor:.2f}\\n\")\n    dokumen_relevan = skor_dokumen.index(max(skor_dokumen))\n    return dokumen_relevan\n# Contoh penggunaan dengan beberapa query yang berbeda",
        "detail": "src.similarity_measure.lexical.experiment",
        "documentation": {}
    },
    {
        "label": "hitung_bobot_query",
        "kind": 2,
        "importPath": "src.similarity_measure.lexical.experiment",
        "description": "src.similarity_measure.lexical.experiment",
        "peekOfCode": "def hitung_bobot_query(query, dokumen, korpus):\n    bobot = 0\n    print(f\"{'Kata':<15}{'TF-IDF':<15}\")\n    for term in query.split():\n        if term in dokumen:\n            tf_idf = hitung_tf_idf(term, dokumen, korpus)\n            bobot += tf_idf * query.count(term) / len(query.split())\n            print(f\"{term:<15}{tf_idf * query.count(term) / len(query.split()):<15.2f}\")\n        else:\n            print(f\"{term:<15}Tidak ditemukan pada dokumen\")",
        "detail": "src.similarity_measure.lexical.experiment",
        "documentation": {}
    },
    {
        "label": "cari_dokumen",
        "kind": 2,
        "importPath": "src.similarity_measure.lexical.experiment",
        "description": "src.similarity_measure.lexical.experiment",
        "peekOfCode": "def cari_dokumen(query, korpus):\n    skor_dokumen = []\n    print(f\"\\nMencari dokumen dengan query '{query}':\")\n    for i, dokumen in enumerate(korpus):\n        skor = hitung_bobot_query(query, dokumen, korpus)\n        skor_dokumen.append(skor)\n        print(f\"Skor dokumen {i+1}: {skor:.2f}\\n\")\n    dokumen_relevan = skor_dokumen.index(max(skor_dokumen))\n    return dokumen_relevan\n# Contoh penggunaan dengan beberapa query yang berbeda",
        "detail": "src.similarity_measure.lexical.experiment",
        "documentation": {}
    },
    {
        "label": "dokumen1",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.experiment",
        "description": "src.similarity_measure.lexical.experiment",
        "peekOfCode": "dokumen1 = \"Saya suka makan nasi goreng\"\ndokumen2 = \"Saya lebih suka makan mie goreng\"\ndokumen3 = \"Nasi goreng adalah makanan favorit saya\"\ndokumen4 = \"Mie goreng adalah makanan kesukaan saya\"\n# Membuat korpus dengan dokumen-dokumen yang sudah dibuat\nkorpus = [dokumen1, dokumen2, dokumen3, dokumen4]\n# Fungsi untuk menghitung pembobotan TF-IDF\ndef hitung_tf_idf(term, dokumen, korpus):\n    tf = dokumen.count(term) / len(dokumen.split())\n    idf = math.log(len(korpus) / (sum([1 for d in korpus if term in d])))",
        "detail": "src.similarity_measure.lexical.experiment",
        "documentation": {}
    },
    {
        "label": "dokumen2",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.experiment",
        "description": "src.similarity_measure.lexical.experiment",
        "peekOfCode": "dokumen2 = \"Saya lebih suka makan mie goreng\"\ndokumen3 = \"Nasi goreng adalah makanan favorit saya\"\ndokumen4 = \"Mie goreng adalah makanan kesukaan saya\"\n# Membuat korpus dengan dokumen-dokumen yang sudah dibuat\nkorpus = [dokumen1, dokumen2, dokumen3, dokumen4]\n# Fungsi untuk menghitung pembobotan TF-IDF\ndef hitung_tf_idf(term, dokumen, korpus):\n    tf = dokumen.count(term) / len(dokumen.split())\n    idf = math.log(len(korpus) / (sum([1 for d in korpus if term in d])))\n    print(f\"{term}\\t\\tTF: {tf:.2f}\\tIDF: {idf:.2f}\")",
        "detail": "src.similarity_measure.lexical.experiment",
        "documentation": {}
    },
    {
        "label": "dokumen3",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.experiment",
        "description": "src.similarity_measure.lexical.experiment",
        "peekOfCode": "dokumen3 = \"Nasi goreng adalah makanan favorit saya\"\ndokumen4 = \"Mie goreng adalah makanan kesukaan saya\"\n# Membuat korpus dengan dokumen-dokumen yang sudah dibuat\nkorpus = [dokumen1, dokumen2, dokumen3, dokumen4]\n# Fungsi untuk menghitung pembobotan TF-IDF\ndef hitung_tf_idf(term, dokumen, korpus):\n    tf = dokumen.count(term) / len(dokumen.split())\n    idf = math.log(len(korpus) / (sum([1 for d in korpus if term in d])))\n    print(f\"{term}\\t\\tTF: {tf:.2f}\\tIDF: {idf:.2f}\")\n    return tf * idf",
        "detail": "src.similarity_measure.lexical.experiment",
        "documentation": {}
    },
    {
        "label": "dokumen4",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.experiment",
        "description": "src.similarity_measure.lexical.experiment",
        "peekOfCode": "dokumen4 = \"Mie goreng adalah makanan kesukaan saya\"\n# Membuat korpus dengan dokumen-dokumen yang sudah dibuat\nkorpus = [dokumen1, dokumen2, dokumen3, dokumen4]\n# Fungsi untuk menghitung pembobotan TF-IDF\ndef hitung_tf_idf(term, dokumen, korpus):\n    tf = dokumen.count(term) / len(dokumen.split())\n    idf = math.log(len(korpus) / (sum([1 for d in korpus if term in d])))\n    print(f\"{term}\\t\\tTF: {tf:.2f}\\tIDF: {idf:.2f}\")\n    return tf * idf\n# Fungsi untuk menghitung nilai bobot dari tiap kata pada suatu dokumen",
        "detail": "src.similarity_measure.lexical.experiment",
        "documentation": {}
    },
    {
        "label": "korpus",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.experiment",
        "description": "src.similarity_measure.lexical.experiment",
        "peekOfCode": "korpus = [dokumen1, dokumen2, dokumen3, dokumen4]\n# Fungsi untuk menghitung pembobotan TF-IDF\ndef hitung_tf_idf(term, dokumen, korpus):\n    tf = dokumen.count(term) / len(dokumen.split())\n    idf = math.log(len(korpus) / (sum([1 for d in korpus if term in d])))\n    print(f\"{term}\\t\\tTF: {tf:.2f}\\tIDF: {idf:.2f}\")\n    return tf * idf\n# Fungsi untuk menghitung nilai bobot dari tiap kata pada suatu dokumen\ndef hitung_bobot_query(query, dokumen, korpus):\n    bobot = 0",
        "detail": "src.similarity_measure.lexical.experiment",
        "documentation": {}
    },
    {
        "label": "query1",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.experiment",
        "description": "src.similarity_measure.lexical.experiment",
        "peekOfCode": "query1 = \"suka makan nasi\"\nquery2 = \"makanan favorit\"\nquery3 = \"mie goreng\"\ndef hitung_bobot_query(query, dokumen, korpus):\n    bobot = 0\n    print(f\"{'Kata':<15}{'TF-IDF':<15}\")\n    for term in query.split():\n        if term in dokumen:\n            tf_idf = hitung_tf_idf(term, dokumen, korpus)\n            bobot += tf_idf * query.count(term) / len(query.split())",
        "detail": "src.similarity_measure.lexical.experiment",
        "documentation": {}
    },
    {
        "label": "query2",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.experiment",
        "description": "src.similarity_measure.lexical.experiment",
        "peekOfCode": "query2 = \"makanan favorit\"\nquery3 = \"mie goreng\"\ndef hitung_bobot_query(query, dokumen, korpus):\n    bobot = 0\n    print(f\"{'Kata':<15}{'TF-IDF':<15}\")\n    for term in query.split():\n        if term in dokumen:\n            tf_idf = hitung_tf_idf(term, dokumen, korpus)\n            bobot += tf_idf * query.count(term) / len(query.split())\n            print(f\"{term:<15}{tf_idf * query.count(term) / len(query.split()):<15.2f}\")",
        "detail": "src.similarity_measure.lexical.experiment",
        "documentation": {}
    },
    {
        "label": "query3",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.experiment",
        "description": "src.similarity_measure.lexical.experiment",
        "peekOfCode": "query3 = \"mie goreng\"\ndef hitung_bobot_query(query, dokumen, korpus):\n    bobot = 0\n    print(f\"{'Kata':<15}{'TF-IDF':<15}\")\n    for term in query.split():\n        if term in dokumen:\n            tf_idf = hitung_tf_idf(term, dokumen, korpus)\n            bobot += tf_idf * query.count(term) / len(query.split())\n            print(f\"{term:<15}{tf_idf * query.count(term) / len(query.split()):<15.2f}\")\n        else:",
        "detail": "src.similarity_measure.lexical.experiment",
        "documentation": {}
    },
    {
        "label": "query1",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.experiment",
        "description": "src.similarity_measure.lexical.experiment",
        "peekOfCode": "query1 = \"suka makan nasi\"\nquery2 = \"makanan favorit\"\nquery3 = \"mie goreng\"\ndokumen_relevan = cari_dokumen(query1, korpus)\nprint(f\"Dokumen yang paling relevan dengan query '{query1}': Dokumen {dokumen_relevan+1}\\n\")\ndokumen_relevan = cari_dokumen(query2, korpus)\nprint(f\"Dokumen yang paling relevan dengan query '{query2}': Dokumen {dokumen_relevan+1}\\n\")\ndokumen_relevan = cari_dokumen(query3, korpus)\nprint(f\"Dokumen yang paling relevan dengan query '{query3}': Dokumen {dokumen_relevan+1}\\n\")",
        "detail": "src.similarity_measure.lexical.experiment",
        "documentation": {}
    },
    {
        "label": "query2",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.experiment",
        "description": "src.similarity_measure.lexical.experiment",
        "peekOfCode": "query2 = \"makanan favorit\"\nquery3 = \"mie goreng\"\ndokumen_relevan = cari_dokumen(query1, korpus)\nprint(f\"Dokumen yang paling relevan dengan query '{query1}': Dokumen {dokumen_relevan+1}\\n\")\ndokumen_relevan = cari_dokumen(query2, korpus)\nprint(f\"Dokumen yang paling relevan dengan query '{query2}': Dokumen {dokumen_relevan+1}\\n\")\ndokumen_relevan = cari_dokumen(query3, korpus)\nprint(f\"Dokumen yang paling relevan dengan query '{query3}': Dokumen {dokumen_relevan+1}\\n\")",
        "detail": "src.similarity_measure.lexical.experiment",
        "documentation": {}
    },
    {
        "label": "query3",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.experiment",
        "description": "src.similarity_measure.lexical.experiment",
        "peekOfCode": "query3 = \"mie goreng\"\ndokumen_relevan = cari_dokumen(query1, korpus)\nprint(f\"Dokumen yang paling relevan dengan query '{query1}': Dokumen {dokumen_relevan+1}\\n\")\ndokumen_relevan = cari_dokumen(query2, korpus)\nprint(f\"Dokumen yang paling relevan dengan query '{query2}': Dokumen {dokumen_relevan+1}\\n\")\ndokumen_relevan = cari_dokumen(query3, korpus)\nprint(f\"Dokumen yang paling relevan dengan query '{query3}': Dokumen {dokumen_relevan+1}\\n\")",
        "detail": "src.similarity_measure.lexical.experiment",
        "documentation": {}
    },
    {
        "label": "dokumen_relevan",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.experiment",
        "description": "src.similarity_measure.lexical.experiment",
        "peekOfCode": "dokumen_relevan = cari_dokumen(query1, korpus)\nprint(f\"Dokumen yang paling relevan dengan query '{query1}': Dokumen {dokumen_relevan+1}\\n\")\ndokumen_relevan = cari_dokumen(query2, korpus)\nprint(f\"Dokumen yang paling relevan dengan query '{query2}': Dokumen {dokumen_relevan+1}\\n\")\ndokumen_relevan = cari_dokumen(query3, korpus)\nprint(f\"Dokumen yang paling relevan dengan query '{query3}': Dokumen {dokumen_relevan+1}\\n\")",
        "detail": "src.similarity_measure.lexical.experiment",
        "documentation": {}
    },
    {
        "label": "dokumen_relevan",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.experiment",
        "description": "src.similarity_measure.lexical.experiment",
        "peekOfCode": "dokumen_relevan = cari_dokumen(query2, korpus)\nprint(f\"Dokumen yang paling relevan dengan query '{query2}': Dokumen {dokumen_relevan+1}\\n\")\ndokumen_relevan = cari_dokumen(query3, korpus)\nprint(f\"Dokumen yang paling relevan dengan query '{query3}': Dokumen {dokumen_relevan+1}\\n\")",
        "detail": "src.similarity_measure.lexical.experiment",
        "documentation": {}
    },
    {
        "label": "dokumen_relevan",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.experiment",
        "description": "src.similarity_measure.lexical.experiment",
        "peekOfCode": "dokumen_relevan = cari_dokumen(query3, korpus)\nprint(f\"Dokumen yang paling relevan dengan query '{query3}': Dokumen {dokumen_relevan+1}\\n\")",
        "detail": "src.similarity_measure.lexical.experiment",
        "documentation": {}
    },
    {
        "label": "LexicalMeasure",
        "kind": 6,
        "importPath": "src.similarity_measure.lexical.lexical",
        "description": "src.similarity_measure.lexical.lexical",
        "peekOfCode": "class LexicalMeasure:\n    def __init__(self, documents, query):\n        self.query = query\n        self.all_ayah_docs = documents\n    def termFrequency(self, term, document):      \n        return document.count(term) / float(len(document))\n    def compute_normalizedtf_all_ayah_docs(self):\n        tf_doc = []\n        for txt in self.all_ayah_docs:\n            sentence = txt['preprocessed']",
        "detail": "src.similarity_measure.lexical.lexical",
        "documentation": {}
    },
    {
        "label": "query",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.lexical",
        "description": "src.similarity_measure.lexical.lexical",
        "peekOfCode": "query = Preprocessing(\"Dengan nama Allah Yang Maha Pengasih, Maha Penyayang.\").execute()\nlexical_measure = LexicalMeasure(get_all_ayahs(), query).execute()\nprint(lexical_measure)",
        "detail": "src.similarity_measure.lexical.lexical",
        "documentation": {}
    },
    {
        "label": "lexical_measure",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.lexical",
        "description": "src.similarity_measure.lexical.lexical",
        "peekOfCode": "lexical_measure = LexicalMeasure(get_all_ayahs(), query).execute()\nprint(lexical_measure)",
        "detail": "src.similarity_measure.lexical.lexical",
        "documentation": {}
    },
    {
        "label": "LexicalMeasure",
        "kind": 6,
        "importPath": "src.similarity_measure.lexical.lexical_measure_manual",
        "description": "src.similarity_measure.lexical.lexical_measure_manual",
        "peekOfCode": "class LexicalMeasure:\n    def __init__(self, documents, query):\n        self.query = query\n        self.all_ayah_docs = documents\n    def termFrequency(self, term, document):      \n        return document.count(term) / float(len(document))\n    def compute_normalizedtf_all_ayah_docs(self):\n        tf_doc = []\n        for txt in self.all_ayah_docs:\n            sentence = txt['preprocessed']",
        "detail": "src.similarity_measure.lexical.lexical_measure_manual",
        "documentation": {}
    },
    {
        "label": "query",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.lexical_measure_manual",
        "description": "src.similarity_measure.lexical.lexical_measure_manual",
        "peekOfCode": "query = \"Dengan nama Allah Yang Maha Pengasih, Maha Penyayang.\"\nquery_preprocessed = Preprocessing(query).execute()\ndocuments = get_all_ayahs()\nlexical_measure_result = LexicalMeasure(documents, query_preprocessed).execute()    \nprint(lexical_measure_result)",
        "detail": "src.similarity_measure.lexical.lexical_measure_manual",
        "documentation": {}
    },
    {
        "label": "query_preprocessed",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.lexical_measure_manual",
        "description": "src.similarity_measure.lexical.lexical_measure_manual",
        "peekOfCode": "query_preprocessed = Preprocessing(query).execute()\ndocuments = get_all_ayahs()\nlexical_measure_result = LexicalMeasure(documents, query_preprocessed).execute()    \nprint(lexical_measure_result)",
        "detail": "src.similarity_measure.lexical.lexical_measure_manual",
        "documentation": {}
    },
    {
        "label": "documents",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.lexical_measure_manual",
        "description": "src.similarity_measure.lexical.lexical_measure_manual",
        "peekOfCode": "documents = get_all_ayahs()\nlexical_measure_result = LexicalMeasure(documents, query_preprocessed).execute()    \nprint(lexical_measure_result)",
        "detail": "src.similarity_measure.lexical.lexical_measure_manual",
        "documentation": {}
    },
    {
        "label": "lexical_measure_result",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.lexical_measure_manual",
        "description": "src.similarity_measure.lexical.lexical_measure_manual",
        "peekOfCode": "lexical_measure_result = LexicalMeasure(documents, query_preprocessed).execute()    \nprint(lexical_measure_result)",
        "detail": "src.similarity_measure.lexical.lexical_measure_manual",
        "documentation": {}
    },
    {
        "label": "termFrequency",
        "kind": 2,
        "importPath": "src.similarity_measure.lexical.tfidf_cosine",
        "description": "src.similarity_measure.lexical.tfidf_cosine",
        "peekOfCode": "def termFrequency(term, document):\n    normalizeDocument = document.lower().split()\n    return normalizeDocument.count(term.lower()) / float(len(normalizeDocument))\ndef compute_normalizedtf(documents):\n    tf_doc = []\n    for txt in documents:\n        sentence = txt.split()\n        norm_tf= dict.fromkeys(set(sentence), 0)\n        for word in sentence:\n            norm_tf[word] = termFrequency(word, txt)",
        "detail": "src.similarity_measure.lexical.tfidf_cosine",
        "documentation": {}
    },
    {
        "label": "compute_normalizedtf",
        "kind": 2,
        "importPath": "src.similarity_measure.lexical.tfidf_cosine",
        "description": "src.similarity_measure.lexical.tfidf_cosine",
        "peekOfCode": "def compute_normalizedtf(documents):\n    tf_doc = []\n    for txt in documents:\n        sentence = txt.split()\n        norm_tf= dict.fromkeys(set(sentence), 0)\n        for word in sentence:\n            norm_tf[word] = termFrequency(word, txt)\n        tf_doc.append(norm_tf)\n        df = pd.DataFrame([norm_tf])\n        idx = 0",
        "detail": "src.similarity_measure.lexical.tfidf_cosine",
        "documentation": {}
    },
    {
        "label": "inverseDocumentFrequency",
        "kind": 2,
        "importPath": "src.similarity_measure.lexical.tfidf_cosine",
        "description": "src.similarity_measure.lexical.tfidf_cosine",
        "peekOfCode": "def inverseDocumentFrequency(term, allDocuments):\n    numDocumentsWithThisTerm = 0\n    for doc in range (0, len(allDocuments)):\n        if term.lower() in allDocuments[doc].lower().split():\n            numDocumentsWithThisTerm = numDocumentsWithThisTerm + 1\n    print(numDocumentsWithThisTerm)\n    if numDocumentsWithThisTerm > 0:\n        return 1.0 + math.log(float(len(allDocuments)) / numDocumentsWithThisTerm)\n    else:\n        return 1.0",
        "detail": "src.similarity_measure.lexical.tfidf_cosine",
        "documentation": {}
    },
    {
        "label": "compute_idf",
        "kind": 2,
        "importPath": "src.similarity_measure.lexical.tfidf_cosine",
        "description": "src.similarity_measure.lexical.tfidf_cosine",
        "peekOfCode": "def compute_idf(documents):\n    idf_dict = {}\n    for doc in documents:\n        sentence = doc.split()\n        for word in sentence:\n            idf_dict[word] = inverseDocumentFrequency(word, documents)\n    # print(idf_dict)            \n    return idf_dict\nidf_dict = compute_idf([doc1, doc2, doc3])\n# tf-idf score across all docs for the query string(\"life learning\")",
        "detail": "src.similarity_measure.lexical.tfidf_cosine",
        "documentation": {}
    },
    {
        "label": "compute_tfidf_with_alldocs",
        "kind": 2,
        "importPath": "src.similarity_measure.lexical.tfidf_cosine",
        "description": "src.similarity_measure.lexical.tfidf_cosine",
        "peekOfCode": "def compute_tfidf_with_alldocs(documents , query):\n    tf_idf = []\n    index = 0\n    query_tokens = query.split()\n    df = pd.DataFrame(columns=['doc'] + query_tokens)\n    for doc in documents:\n        df['doc'] = np.arange(0 , len(documents))\n        doc_num = tf_doc[index]\n        sentence = doc.split()\n        for word in sentence:",
        "detail": "src.similarity_measure.lexical.tfidf_cosine",
        "documentation": {}
    },
    {
        "label": "compute_query_tf",
        "kind": 2,
        "importPath": "src.similarity_measure.lexical.tfidf_cosine",
        "description": "src.similarity_measure.lexical.tfidf_cosine",
        "peekOfCode": "def compute_query_tf(query):\n    query_norm_tf = {}\n    tokens = query.split()\n    for word in tokens:\n        query_norm_tf[word] = termFrequency(word , query)\n    return query_norm_tf\nquery_norm_tf = compute_query_tf(query)\n# print(\"Normalized TF for the query string : \",query_norm_tf)\n#idf score for the query string(\"life learning\")\ndef compute_query_idf(query):",
        "detail": "src.similarity_measure.lexical.tfidf_cosine",
        "documentation": {}
    },
    {
        "label": "compute_query_idf",
        "kind": 2,
        "importPath": "src.similarity_measure.lexical.tfidf_cosine",
        "description": "src.similarity_measure.lexical.tfidf_cosine",
        "peekOfCode": "def compute_query_idf(query):\n    idf_dict_qry = {}\n    sentence = query.split()\n    documents = [doc1, doc2, doc3]\n    for word in sentence:\n        idf_dict_qry[word] = inverseDocumentFrequency(word ,documents)\n    return idf_dict_qry\nidf_dict_qry = compute_query_idf(query)\n# print(\"IDF score for the query string : \",idf_dict_qry)\n#tf-idf score for the query string(\"life learning\")",
        "detail": "src.similarity_measure.lexical.tfidf_cosine",
        "documentation": {}
    },
    {
        "label": "compute_query_tfidf",
        "kind": 2,
        "importPath": "src.similarity_measure.lexical.tfidf_cosine",
        "description": "src.similarity_measure.lexical.tfidf_cosine",
        "peekOfCode": "def compute_query_tfidf(query):\n    tfidf_dict_qry = {}\n    sentence = query.split()\n    for word in sentence:\n        tfidf_dict_qry[word] = query_norm_tf[word] * idf_dict_qry[word]\n    return tfidf_dict_qry\n# tfidf_dict_qry = compute_query_tfidf(query)\n# print(\"TF-IDF score for the query string : \",tfidf_dict_qry)\n#Cosine Similarity(Query,Document1) = Dot product(Query, Document1) / ||Query|| * ||Document1||\n\"\"\"",
        "detail": "src.similarity_measure.lexical.tfidf_cosine",
        "documentation": {}
    },
    {
        "label": "cosine_similarity",
        "kind": 2,
        "importPath": "src.similarity_measure.lexical.tfidf_cosine",
        "description": "src.similarity_measure.lexical.tfidf_cosine",
        "peekOfCode": "def cosine_similarity(tfidf_dict_qry, df , query , doc_num):\n    dot_product = 0\n    qry_mod = 0\n    doc_mod = 0\n    tokens = query.split()\n    for keyword in tokens:\n        dot_product += tfidf_dict_qry[keyword] * df[keyword][df['doc'] == doc_num]\n        #||Query||\n        qry_mod += tfidf_dict_qry[keyword] * tfidf_dict_qry[keyword]\n        #||Document||",
        "detail": "src.similarity_measure.lexical.tfidf_cosine",
        "documentation": {}
    },
    {
        "label": "doc1",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.tfidf_cosine",
        "description": "src.similarity_measure.lexical.tfidf_cosine",
        "peekOfCode": "doc1 = \"I want to start learning to charge something in life\"\ndoc2 = \"reading something about life no one else knows\"\ndoc3 = \"Never stop learning\"\n#query string\nquery = \"life learning\"\n#term-frequency :word occurences in a document\n# def compute_tf(docs_list):\n#     for doc in docs_list:\n#         doc1_lst = doc.split(\" \")\n#         wordDict_1= dict.fromkeys(set(doc1_lst), 0)",
        "detail": "src.similarity_measure.lexical.tfidf_cosine",
        "documentation": {}
    },
    {
        "label": "doc2",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.tfidf_cosine",
        "description": "src.similarity_measure.lexical.tfidf_cosine",
        "peekOfCode": "doc2 = \"reading something about life no one else knows\"\ndoc3 = \"Never stop learning\"\n#query string\nquery = \"life learning\"\n#term-frequency :word occurences in a document\n# def compute_tf(docs_list):\n#     for doc in docs_list:\n#         doc1_lst = doc.split(\" \")\n#         wordDict_1= dict.fromkeys(set(doc1_lst), 0)\n#         for token in doc1_lst:",
        "detail": "src.similarity_measure.lexical.tfidf_cosine",
        "documentation": {}
    },
    {
        "label": "doc3",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.tfidf_cosine",
        "description": "src.similarity_measure.lexical.tfidf_cosine",
        "peekOfCode": "doc3 = \"Never stop learning\"\n#query string\nquery = \"life learning\"\n#term-frequency :word occurences in a document\n# def compute_tf(docs_list):\n#     for doc in docs_list:\n#         doc1_lst = doc.split(\" \")\n#         wordDict_1= dict.fromkeys(set(doc1_lst), 0)\n#         for token in doc1_lst:\n#             wordDict_1[token] +=  1",
        "detail": "src.similarity_measure.lexical.tfidf_cosine",
        "documentation": {}
    },
    {
        "label": "query",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.tfidf_cosine",
        "description": "src.similarity_measure.lexical.tfidf_cosine",
        "peekOfCode": "query = \"life learning\"\n#term-frequency :word occurences in a document\n# def compute_tf(docs_list):\n#     for doc in docs_list:\n#         doc1_lst = doc.split(\" \")\n#         wordDict_1= dict.fromkeys(set(doc1_lst), 0)\n#         for token in doc1_lst:\n#             wordDict_1[token] +=  1\n#         df = pd.DataFrame([wordDict_1])\n#         idx = 0",
        "detail": "src.similarity_measure.lexical.tfidf_cosine",
        "documentation": {}
    },
    {
        "label": "tf_doc",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.tfidf_cosine",
        "description": "src.similarity_measure.lexical.tfidf_cosine",
        "peekOfCode": "tf_doc = compute_normalizedtf([doc1, doc2, doc3])\ndef inverseDocumentFrequency(term, allDocuments):\n    numDocumentsWithThisTerm = 0\n    for doc in range (0, len(allDocuments)):\n        if term.lower() in allDocuments[doc].lower().split():\n            numDocumentsWithThisTerm = numDocumentsWithThisTerm + 1\n    print(numDocumentsWithThisTerm)\n    if numDocumentsWithThisTerm > 0:\n        return 1.0 + math.log(float(len(allDocuments)) / numDocumentsWithThisTerm)\n    else:",
        "detail": "src.similarity_measure.lexical.tfidf_cosine",
        "documentation": {}
    },
    {
        "label": "idf_dict",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.tfidf_cosine",
        "description": "src.similarity_measure.lexical.tfidf_cosine",
        "peekOfCode": "idf_dict = compute_idf([doc1, doc2, doc3])\n# tf-idf score across all docs for the query string(\"life learning\")\ndef compute_tfidf_with_alldocs(documents , query):\n    tf_idf = []\n    index = 0\n    query_tokens = query.split()\n    df = pd.DataFrame(columns=['doc'] + query_tokens)\n    for doc in documents:\n        df['doc'] = np.arange(0 , len(documents))\n        doc_num = tf_doc[index]",
        "detail": "src.similarity_measure.lexical.tfidf_cosine",
        "documentation": {}
    },
    {
        "label": "documents",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.tfidf_cosine",
        "description": "src.similarity_measure.lexical.tfidf_cosine",
        "peekOfCode": "documents = [doc1, doc2, doc3]\ntf_idf , df = compute_tfidf_with_alldocs(documents , query)\n# print(df)\n#Normalized TF for the query string(\"life learning\")\ndef compute_query_tf(query):\n    query_norm_tf = {}\n    tokens = query.split()\n    for word in tokens:\n        query_norm_tf[word] = termFrequency(word , query)\n    return query_norm_tf",
        "detail": "src.similarity_measure.lexical.tfidf_cosine",
        "documentation": {}
    },
    {
        "label": "query_norm_tf",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.tfidf_cosine",
        "description": "src.similarity_measure.lexical.tfidf_cosine",
        "peekOfCode": "query_norm_tf = compute_query_tf(query)\n# print(\"Normalized TF for the query string : \",query_norm_tf)\n#idf score for the query string(\"life learning\")\ndef compute_query_idf(query):\n    idf_dict_qry = {}\n    sentence = query.split()\n    documents = [doc1, doc2, doc3]\n    for word in sentence:\n        idf_dict_qry[word] = inverseDocumentFrequency(word ,documents)\n    return idf_dict_qry",
        "detail": "src.similarity_measure.lexical.tfidf_cosine",
        "documentation": {}
    },
    {
        "label": "idf_dict_qry",
        "kind": 5,
        "importPath": "src.similarity_measure.lexical.tfidf_cosine",
        "description": "src.similarity_measure.lexical.tfidf_cosine",
        "peekOfCode": "idf_dict_qry = compute_query_idf(query)\n# print(\"IDF score for the query string : \",idf_dict_qry)\n#tf-idf score for the query string(\"life learning\")\ndef compute_query_tfidf(query):\n    tfidf_dict_qry = {}\n    sentence = query.split()\n    for word in sentence:\n        tfidf_dict_qry[word] = query_norm_tf[word] * idf_dict_qry[word]\n    return tfidf_dict_qry\n# tfidf_dict_qry = compute_query_tfidf(query)",
        "detail": "src.similarity_measure.lexical.tfidf_cosine",
        "documentation": {}
    },
    {
        "label": "hapus_nomor_ayat",
        "kind": 2,
        "importPath": "src.similarity_measure.lexical.tf_idf_experiments",
        "description": "src.similarity_measure.lexical.tf_idf_experiments",
        "peekOfCode": "def hapus_nomor_ayat(text):\n    text = text.replace(\"|\",\"\")\n    text = text.replace(\"nan\",'')\n    text = re.sub(\"\\d+\", \"\", text)\n    return text\n# Menghapus kata berulang, contoh: orang-orang\ndef kataBerulang(text):\n    words = word_tokenize(text)\n    new = []\n    for word in words:",
        "detail": "src.similarity_measure.lexical.tf_idf_experiments",
        "documentation": {}
    },
    {
        "label": "kataBerulang",
        "kind": 2,
        "importPath": "src.similarity_measure.lexical.tf_idf_experiments",
        "description": "src.similarity_measure.lexical.tf_idf_experiments",
        "peekOfCode": "def kataBerulang(text):\n    words = word_tokenize(text)\n    new = []\n    for word in words:\n        sep = '-'\n        stripped = word.split(sep, 1)[0]\n        new.append(stripped)\n    return \" \".join(new)\n# Menghapus tanda baca dan lower case semua kata\ndef preprocessing_indo(teks):",
        "detail": "src.similarity_measure.lexical.tf_idf_experiments",
        "documentation": {}
    },
    {
        "label": "preprocessing_indo",
        "kind": 2,
        "importPath": "src.similarity_measure.lexical.tf_idf_experiments",
        "description": "src.similarity_measure.lexical.tf_idf_experiments",
        "peekOfCode": "def preprocessing_indo(teks):\n    teks = teks.strip()\n    teks = re.sub(r'\\([^)]*\\)', '', teks) #Hapus tanda kurung\n    teks = teks.translate(str.maketrans('','', string.punctuation))\n    teks = teks.lower()\n    return teks\n# Mengembalikan Kata ke dalam bentuk aslinya\ndef indo_stemming(text):\n    factory = StemmerFactory()\n    stemmer = factory.create_stemmer()",
        "detail": "src.similarity_measure.lexical.tf_idf_experiments",
        "documentation": {}
    },
    {
        "label": "indo_stemming",
        "kind": 2,
        "importPath": "src.similarity_measure.lexical.tf_idf_experiments",
        "description": "src.similarity_measure.lexical.tf_idf_experiments",
        "peekOfCode": "def indo_stemming(text):\n    factory = StemmerFactory()\n    stemmer = factory.create_stemmer()\n    words = word_tokenize(text)\n    needed_words = []\n    for w in words:\n        needed_words.append(stemmer.stem(w))\n        filtered_sentence = \" \".join(needed_words)\n    return filtered_sentence\n# Menghilangkan kata yang tidak berguna",
        "detail": "src.similarity_measure.lexical.tf_idf_experiments",
        "documentation": {}
    },
    {
        "label": "indo_stopWordRemove",
        "kind": 2,
        "importPath": "src.similarity_measure.lexical.tf_idf_experiments",
        "description": "src.similarity_measure.lexical.tf_idf_experiments",
        "peekOfCode": "def indo_stopWordRemove(text):\n    stop_factory = StopWordRemoverFactory()\n    stopword = stop_factory.get_stop_words()\n    tokenized = word_tokenize(text)\n    new = []\n    for word in tokenized:\n        if word in stopword:\n            None\n        else:\n            new.append(word)",
        "detail": "src.similarity_measure.lexical.tf_idf_experiments",
        "documentation": {}
    },
    {
        "label": "indo_final_preprocess",
        "kind": 2,
        "importPath": "src.similarity_measure.lexical.tf_idf_experiments",
        "description": "src.similarity_measure.lexical.tf_idf_experiments",
        "peekOfCode": "def indo_final_preprocess(text):\n    text = kataBerulang(text)\n    text = hapus_nomor_ayat(text)\n    text = preprocessing_indo(text)\n    text = indo_stemming(text)\n    text = indo_stopWordRemove(text)\n    return text\nprint(indo_final_preprocess(\"Segala puji bagi Allah Tuhan semesta alam.\"))",
        "detail": "src.similarity_measure.lexical.tf_idf_experiments",
        "documentation": {}
    },
    {
        "label": "root",
        "kind": 2,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "def root():\n    return {\"message\": \"Welcome to Quran Finder API. Please go to /docs for more info.\"}\n@app.get(\"/test-connection\")\nasync def test_connections():\n    connections_status = test_connections()\n    if(connections_status):\n        return {\"message\": \"Quran Finder API is running. \"}\n    return {\"message\": \"Quran Finder API is not running. \"}\n@app.get(\"/all-surahs\")\nasync def get_all_surahs():",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "app",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "app = FastAPI()\napp.title = (\n    \"Quran Search API for searching Quranic verses with lexical and semantic features\"\n)\napp.version = \"0.0.1\"\napp.debug = True\n@app.get(\"/\")\ndef root():\n    return {\"message\": \"Welcome to Quran Finder API. Please go to /docs for more info.\"}\n@app.get(\"/test-connection\")",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "app.title",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "app.title = (\n    \"Quran Search API for searching Quranic verses with lexical and semantic features\"\n)\napp.version = \"0.0.1\"\napp.debug = True\n@app.get(\"/\")\ndef root():\n    return {\"message\": \"Welcome to Quran Finder API. Please go to /docs for more info.\"}\n@app.get(\"/test-connection\")\nasync def test_connections():",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "app.version",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "app.version = \"0.0.1\"\napp.debug = True\n@app.get(\"/\")\ndef root():\n    return {\"message\": \"Welcome to Quran Finder API. Please go to /docs for more info.\"}\n@app.get(\"/test-connection\")\nasync def test_connections():\n    connections_status = test_connections()\n    if(connections_status):\n        return {\"message\": \"Quran Finder API is running. \"}",
        "detail": "main",
        "documentation": {}
    },
    {
        "label": "app.debug",
        "kind": 5,
        "importPath": "main",
        "description": "main",
        "peekOfCode": "app.debug = True\n@app.get(\"/\")\ndef root():\n    return {\"message\": \"Welcome to Quran Finder API. Please go to /docs for more info.\"}\n@app.get(\"/test-connection\")\nasync def test_connections():\n    connections_status = test_connections()\n    if(connections_status):\n        return {\"message\": \"Quran Finder API is running. \"}\n    return {\"message\": \"Quran Finder API is not running. \"}",
        "detail": "main",
        "documentation": {}
    }
]